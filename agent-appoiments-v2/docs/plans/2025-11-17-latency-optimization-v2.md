# LangGraph Agent Latency Optimization Plan (v2.0 - CORRECTED)

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Reduce agent latency from 7.2 iterations/3.9s average to 2-3 iterations/1.5-2s while maintaining functionality

**Architecture:** Production-grade async patterns with LangGraph native features (recursion_limit, streaming, prebuilt agents)

**Tech Stack:** LangGraph 1.0, LangChain, OpenAI GPT-4o-mini, Python 3.11+ with async/await

**Current Baseline Metrics:**
- Average latency: 3,860ms (~3.9s)
- Average iterations: 7.2
- LLM time: 47% (1,660ms per call)
- Tools time: 28.5% (but mostly fast, 1-10ms)
- Target: 1,500-2,000ms with 2-3 iterations

**Critical Fixes vs v1.0:**
- ‚úÖ Use LangGraph native `recursion_limit` (not manual iteration_count)
- ‚úÖ Async patterns throughout (ainvoke, astream, async def)
- ‚úÖ Proper streaming with `stream_mode="updates"`
- ‚úÖ Evaluate `create_react_agent` FIRST (not as optional fallback)
- ‚úÖ Production-ready concurrent request handling

---

## üìä PHASE 1: ARCHITECTURE DECISIONS (2 hours)

### Task 1: System Prompt Re-Engineering for Efficiency

**Goal:** Reduce iterations from 7.2 to ~5 by making LLM more decisive

**Files:**
- Modify: `agent-appoiments-v2/src/agent.py:83-188` (build_system_prompt function)

**Step 1: Create async baseline measurement script**

Create `agent-appoiments-v2/scripts/measure_baseline.py`:

```python
"""Baseline measurement script (ASYNC version)."""
import os
import asyncio
from langchain_smith import Client
from datetime import datetime, timedelta
import statistics

async def measure_recent_runs(hours=24, limit=50):
    """Measure metrics from recent LangSmith runs."""
    client = Client()

    # Get runs from last N hours
    end_time = datetime.now()
    start_time = end_time - timedelta(hours=hours)

    runs = list(client.list_runs(
        project_name=os.getenv("LANGCHAIN_PROJECT", "appointment-agent-v2"),
        start_time=start_time,
        end_time=end_time,
        is_root=True,
        limit=limit
    ))

    latencies = []
    iterations = []

    for run in runs:
        if run.status == "success" and run.latency:
            latencies.append(run.latency)
            # Count child runs as iterations
            child_count = len(list(client.list_runs(parent_run_id=run.id)))
            iterations.append(child_count)

    if not latencies:
        print("‚ùå No runs found in specified time window")
        return None

    metrics = {
        "count": len(latencies),
        "avg_latency_ms": statistics.mean(latencies),
        "median_latency_ms": statistics.median(latencies),
        "avg_iterations": statistics.mean(iterations) if iterations else 0,
        "max_iterations": max(iterations) if iterations else 0,
        "timestamp": datetime.now().isoformat()
    }

    print(f"üìä Baseline Metrics from {len(latencies)} runs:")
    print(f"   Avg Latency: {metrics['avg_latency_ms']:.0f}ms")
    print(f"   Median Latency: {metrics['median_latency_ms']:.0f}ms")
    print(f"   Avg Iterations: {metrics['avg_iterations']:.1f}")
    print(f"   Max Iterations: {metrics['max_iterations']}")

    return metrics

if __name__ == "__main__":
    asyncio.run(measure_recent_runs(hours=24, limit=100))
```

**Step 2: Run baseline measurement**

Run: `cd agent-appoiments-v2 && source venv/bin/activate && python scripts/measure_baseline.py`
Expected: Output showing current metrics (~3.9s, 7.2 iterations)

**Step 3: Optimize system prompt (same as v1 - this part was correct)**

Replace `build_system_prompt()` in `src/agent.py:83-188` with imperative, efficiency-focused version:

```python
def build_system_prompt(state: AppointmentState) -> str:
    """
    Build context-aware system prompt (v2.0 - EFFICIENCY OPTIMIZED).

    Changes from v1.10:
    - Imperative commands instead of descriptive text
    - Explicit parallel tool calling instructions
    - Decision trees for common paths
    - Removal of ambiguity

    Optimization goal: Reduce from 7.2 to 3-4 iterations.
    """
    current = state.get("current_state", ConversationState.COLLECT_SERVICE)

    # CORE RULES (~80 tokens, ultra-compressed)
    base = """Efficient booking agent. USER'S LANGUAGE. 1 Q/time.
IMPERATIVE: Use MULTIPLE tools in ONE response when possible.
UPDATE: collected_data{} + current_state EVERY turn.
NEVER: fake data, skip validation, assume anything.

TOOL COMBOS (use together):
- Service selected ‚Üí fetch_cache(svc_id) + ask time pref [PARALLEL]
- User gives email+phone ‚Üí validate_email(email) + validate_phone(phone) [PARALLEL]
- Availability shown + user picks ‚Üí DIRECT to create_appointment [NO EXTRA CONFIRMATION]

FLOWS: Book|Cancel|Reschedule
SECURITY: conf# only for cancel/reschedule"""

    # STATE-SPECIFIC DIRECTIVES (ultra-compressed)
    states = {
        ConversationState.COLLECT_SERVICE:
            "ACTION: get_services() + show list ‚Üí user picks ‚Üí fetch_cache(svc_id) SILENT + ask 'morning/afternoon/any?'",

        ConversationState.COLLECT_TIME_PREFERENCE:
            "ACTION: filter_show(svc_id, time_pref='morning'|'afternoon'|'any', offset=0) ‚Üí show 3 days",

        ConversationState.SHOW_AVAILABILITY:
            "SHOWN: 3 days. User wants more? filter_show(..., offset+3). User picks slot? ‚Üí COLLECT NAME (no confirmation yet)",

        ConversationState.COLLECT_NAME:
            "ASK: 'Full name?' WAIT. NO defaults.",

        ConversationState.COLLECT_EMAIL:
            "ASK: 'Email?' WAIT ‚Üí validate_email(input). INVALID? Re-ask. VALID? Next.",

        ConversationState.COLLECT_PHONE:
            "ASK: 'Phone?' WAIT ‚Üí validate_phone(input). INVALID? Re-ask. VALID? ‚Üí SUMMARY",

        ConversationState.SHOW_SUMMARY:
            "SHOW: svc, date, time, name, email, phone, provider, location. ASK: 'Confirm?' [yes/no only]",

        ConversationState.CONFIRM:
            "WAIT yes/no. YES ‚Üí CREATE_APPOINTMENT. NO ‚Üí ask 'What to change?'",

        ConversationState.CREATE_APPOINTMENT:
            "NOW: create_appointment_tool(svc_id, date, start_time, name, email, phone) with REAL data. SHOW conf# prominently.",

        ConversationState.COMPLETE:
            "‚úÖ Done. Show conf#. Ask 'Anything else?'",

        ConversationState.CANCEL_ASK_CONFIRMATION:
            "ASK: 'Confirmation number?' (format: APPT-12345)",

        ConversationState.CANCEL_VERIFY:
            "ACTION: cancel_appointment_tool(conf#). SUCCESS? ‚Üí Confirm. ERROR? verify again (max 2√ó)",

        ConversationState.CANCEL_CONFIRM:
            "SHOW details. ASK: 'Cancel this appointment?' yes/no",

        ConversationState.CANCEL_PROCESS:
            "ACTION: cancel_appointment_tool(conf#) ‚Üí show confirmation",

        ConversationState.RESCHEDULE_ASK_CONFIRMATION:
            "ASK: 'Confirmation number?'",

        ConversationState.RESCHEDULE_VERIFY:
            "ACTION: get_appointment_tool(conf#). SUCCESS? show current + ask new time. ERROR? retry (max 2√ó)",

        ConversationState.RESCHEDULE_SELECT_DATETIME:
            "ACTION: fetch_cache(svc_id) + filter_show(...) [PARALLEL] ‚Üí show slots. User picks ‚Üí CONFIRM",

        ConversationState.RESCHEDULE_CONFIRM:
            "SHOW: old ‚Üí new. ASK: 'Confirm reschedule?'",

        ConversationState.RESCHEDULE_PROCESS:
            "ACTION: reschedule_appointment_tool(conf#, new_date, new_time) ‚Üí done",

        ConversationState.POST_ACTION:
            "ASK: 'Need anything else? Book|Cancel|Reschedule'",
    }

    current_val = current.value if hasattr(current, 'value') else current
    directive = states.get(current, f"STATE: {current_val}")

    return f"{base}\n\nCURRENT_STATE: {directive}"
```

**Step 4: Run tests**

Run: `cd agent-appoiments-v2 && source venv/bin/activate && timeout 180 pytest tests/challenge/test_1_complete_flows.py -v --tb=short`
Expected: All tests pass

**Step 5: Commit**

```bash
git add src/agent.py scripts/measure_baseline.py
git commit -m "feat(optimization): re-engineer system prompt for efficiency

- Add imperative commands for decisive LLM behavior
- Explicit parallel tool calling instructions
- Async measurement script
- Expected: 7.2 ‚Üí 5 iterations (30% reduction)

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 2: Evaluate create_react_agent (FIRST, not optional)

**Goal:** Determine if LangGraph's prebuilt agent meets requirements before building custom optimizations

**WHY THIS IS TASK 2 (not Task 7):**
- ‚úÖ Already optimized by LangGraph team
- ‚úÖ Parallel tool calling native
- ‚úÖ Retry logic included
- ‚úÖ Less code = fewer bugs
- ‚úÖ If it works, saves 2 days of custom optimization

**Files:**
- Create: `agent-appoiments-v2/src/agent_prebuilt.py`
- Create: `tests/test_prebuilt_evaluation.py`

**Step 1: Create prebuilt agent implementation**

Create `src/agent_prebuilt.py`:

```python
"""
Prebuilt agent using create_react_agent (v2.0 - EVALUATION).

This is the RECOMMENDED starting point before custom optimization.
LangGraph's prebuilt agent includes battle-tested optimizations.

Evaluation criteria:
- ‚úÖ Meets functional requirements?
- ‚úÖ Latency competitive with custom?
- ‚úÖ Handles all flows (booking, cancel, reschedule)?
- ‚úÖ Compatible with existing state?

If YES to all ‚Üí Use this. If NO ‚Üí Continue with custom optimization.
"""
import os
from typing import Annotated, Optional
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# Import existing tools
from src.tools import (
    validate_email_tool,
    validate_phone_tool,
    get_services_tool,
    fetch_and_cache_availability_tool,
    filter_and_show_availability_tool,
    create_appointment_tool
)
from src.tools_appointment_mgmt import (
    cancel_appointment_tool,
    get_appointment_tool,
    reschedule_appointment_tool,
)

load_dotenv()

# Simplified state for prebuilt agent
class PrebuiltAgentState(TypedDict):
    """
    State compatible with create_react_agent.

    NOTE: Prebuilt agent doesn't support custom state fields like current_state.
    All logic must be in system prompt.
    """
    messages: Annotated[list, add_messages]


def build_prebuilt_system_prompt() -> str:
    """
    Single comprehensive prompt for prebuilt agent.

    Must encode ALL state logic since we can't use dynamic current_state.
    This is the main trade-off vs custom implementation.
    """
    return """You are an efficient appointment booking assistant.

**CORE EFFICIENCY RULES:**
- Respond in user's language
- Ask ONE question at a time
- Use MULTIPLE tools in PARALLEL whenever possible
- NEVER use fake/default data
- Complete booking in 2-3 turns (not 7+)

**PARALLEL TOOL USAGE (CRITICAL FOR SPEED):**
- User selects service ‚Üí fetch_and_cache(svc_id) + ask time preference [BOTH IN SAME RESPONSE]
- User gives "email@test.com and 555-1234" ‚Üí validate_email() + validate_phone() [BOTH IN SAME RESPONSE]
- When showing availability and user picks slot ‚Üí CREATE appointment directly [NO EXTRA CONFIRMATION STEP]

**BOOKING FLOW (STREAMLINED):**
1. get_services() ‚Üí user picks service
2. fetch_and_cache(svc_id) SILENTLY (no message to user) + ask "morning, afternoon, or any time?"
3. filter_and_show(svc_id, time_pref, offset=0) ‚Üí show 3 days of slots
4. User picks date/time ‚Üí ask full name
5. Ask email ‚Üí validate_email() ‚Üí if INVALID re-ask immediately
6. Ask phone ‚Üí validate_phone() ‚Üí if INVALID re-ask immediately
7. Show summary (service, date, time, name, email, phone) ‚Üí ask "Confirm? (yes/no)"
8. User confirms ‚Üí create_appointment_tool(ALL DATA) ‚Üí show confirmation number prominently
9. Done ‚Üí ask "Anything else?"

**CANCEL FLOW:**
1. Ask for confirmation number (format: APPT-12345)
2. get_appointment_tool(conf#) ‚Üí show details
3. Ask "Cancel this appointment? (yes/no)"
4. User confirms ‚Üí cancel_appointment_tool(conf#) ‚Üí done

**RESCHEDULE FLOW:**
1. Ask for confirmation number
2. get_appointment_tool(conf#) ‚Üí show current appointment
3. fetch_and_cache(svc_id) + filter_and_show() ‚Üí show new slots
4. User picks new date/time ‚Üí show old vs new
5. Ask "Confirm reschedule?"
6. User confirms ‚Üí reschedule_appointment_tool(conf#, new_date, new_time) ‚Üí done

**SECURITY:**
- ONLY use confirmation numbers for cancel/reschedule
- NEVER look up appointments by email or phone (security risk)

**EFFICIENCY IMPERATIVE:**
Your goal is to complete each conversation in 2-3 agent turns (not 7+).
Use parallel tools aggressively. Skip unnecessary confirmation steps."""


def create_prebuilt_graph():
    """
    Create agent using LangGraph's optimized create_react_agent.

    This is the RECOMMENDED approach before custom optimization.

    Returns:
        Compiled prebuilt agent graph
    """
    # Create LLM with parallel tools enabled
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.2,
        max_tokens=200,
        timeout=15,
        request_timeout=15,
        api_key=os.getenv("OPENAI_API_KEY"),
        model_kwargs={
            "parallel_tool_calls": True  # ‚Üê CRITICAL for efficiency
        }
    )

    # Assemble tools list
    tools = [
        # Booking tools
        get_services_tool,
        fetch_and_cache_availability_tool,
        filter_and_show_availability_tool,
        validate_email_tool,
        validate_phone_tool,
        create_appointment_tool,
        # Management tools
        cancel_appointment_tool,
        get_appointment_tool,
        reschedule_appointment_tool,
    ]

    # Build system prompt
    system_prompt = build_prebuilt_system_prompt()

    # Create prebuilt agent with optimizations
    graph = create_react_agent(
        model=llm,
        tools=tools,
        state_schema=PrebuiltAgentState,
        state_modifier=system_prompt,  # Injects system prompt
        checkpointer=MemorySaver(),
    )

    return graph
```

**Step 2: Create comprehensive evaluation test**

Create `tests/test_prebuilt_evaluation.py`:

```python
"""
Evaluation test for prebuilt agent vs custom agent.

This test determines if we should use prebuilt or continue custom optimization.
"""
import pytest
import time
import asyncio
from langchain_core.messages import HumanMessage
from src.agent import create_graph as create_custom_graph
from src.agent_prebuilt import create_prebuilt_graph

@pytest.mark.asyncio
async def test_prebuilt_functional_booking():
    """Test that prebuilt agent can complete full booking flow."""
    graph = create_prebuilt_graph()
    config = {"configurable": {"thread_id": "eval-booking", "recursion_limit": 10}}

    # Simulate full booking conversation
    messages = [
        "I want to book an appointment",
        "General Consultation",
        "Morning",
        "Tomorrow at 9am",
        "John Doe",
        "john@example.com",
        "5551234567",
        "yes"
    ]

    state = {"messages": []}
    for msg in messages:
        state["messages"].append(HumanMessage(content=msg))
        result = graph.invoke(state, config)
        state = result

    # Verify booking completed
    last_message = str(result["messages"][-1].content)
    assert "APPT-" in last_message or "confirmation" in last_message.lower(), \
        "Booking did not complete with confirmation number"

    print("‚úÖ Prebuilt agent: Booking flow works")


@pytest.mark.asyncio
async def test_prebuilt_functional_cancellation():
    """Test that prebuilt agent can cancel appointments."""
    graph = create_prebuilt_graph()
    config = {"configurable": {"thread_id": "eval-cancel", "recursion_limit": 10}}

    # First book, then cancel
    # (Simplified test - assumes appointment exists)
    messages = [
        "Cancel my appointment",
        "APPT-12345"
    ]

    state = {"messages": []}
    for msg in messages:
        state["messages"].append(HumanMessage(content=msg))
        result = graph.invoke(state, config)
        state = result

    # Should ask for confirmation or show appointment details
    assert len(result["messages"]) > len(messages)
    print("‚úÖ Prebuilt agent: Cancellation flow works")


@pytest.mark.performance
@pytest.mark.asyncio
async def test_compare_prebuilt_vs_custom_performance():
    """
    CRITICAL TEST: Compare prebuilt vs custom agent performance.

    Decision criteria:
    - If prebuilt is within 20% of custom ‚Üí USE PREBUILT (less maintenance)
    - If prebuilt is >20% slower ‚Üí CONTINUE CUSTOM OPTIMIZATION
    """
    custom_graph = create_custom_graph()
    prebuilt_graph = create_prebuilt_graph()

    test_flow = [
        "Book appointment",
        "General Consultation",
        "Morning",
        "Tomorrow at 9am",
        "Jane Smith",
        "jane@example.com",
        "5559876543",
        "yes"
    ]

    # Test custom agent
    custom_config = {"configurable": {"thread_id": "perf-custom", "recursion_limit": 10}}
    custom_state = {"messages": []}
    start = time.time()
    for msg in test_flow:
        custom_state["messages"].append(HumanMessage(content=msg))
        result_custom = custom_graph.invoke(custom_state, custom_config)
        custom_state = result_custom
    custom_time = time.time() - start

    # Test prebuilt agent
    prebuilt_config = {"configurable": {"thread_id": "perf-prebuilt", "recursion_limit": 10}}
    prebuilt_state = {"messages": []}
    start = time.time()
    for msg in test_flow:
        prebuilt_state["messages"].append(HumanMessage(content=msg))
        result_prebuilt = prebuilt_graph.invoke(prebuilt_state, prebuilt_config)
        prebuilt_state = result_prebuilt
    prebuilt_time = time.time() - start

    # Calculate difference
    diff_pct = ((custom_time / prebuilt_time - 1) * 100) if prebuilt_time > 0 else 0

    print("\n" + "="*80)
    print("üìä PREBUILT vs CUSTOM PERFORMANCE COMPARISON")
    print("="*80)
    print(f"Custom Agent:   {custom_time:.2f}s")
    print(f"Prebuilt Agent: {prebuilt_time:.2f}s")
    print(f"Difference:     {abs(diff_pct):.1f}% ({'prebuilt faster' if prebuilt_time < custom_time else 'custom faster'})")
    print("="*80)

    # Decision criteria
    if prebuilt_time <= custom_time * 1.2:  # Within 20%
        print("\n‚úÖ RECOMMENDATION: USE PREBUILT AGENT")
        print("   Reasons:")
        print("   - Performance competitive with custom")
        print("   - Less code to maintain")
        print("   - Battle-tested optimizations")
        print("   - Automatic updates from LangGraph")
    else:
        print("\n‚ö†Ô∏è  RECOMMENDATION: CONTINUE CUSTOM OPTIMIZATION")
        print("   Reasons:")
        print(f"   - Prebuilt is {diff_pct:.1f}% slower (>20% threshold)")
        print("   - Custom optimization likely to yield better results")

    # Informational only - no hard assertion
    # Team decides based on printed recommendation


@pytest.mark.asyncio
async def test_prebuilt_state_compatibility():
    """Test if prebuilt agent's state is compatible with our needs."""
    graph = create_prebuilt_graph()
    config = {"configurable": {"thread_id": "eval-state", "recursion_limit": 10}}

    state = {"messages": [HumanMessage(content="Hello")]}
    result = graph.invoke(state, config)

    # Check state structure
    assert "messages" in result
    assert len(result["messages"]) > 1

    # Check if we can extract conversation context
    # (Prebuilt doesn't have current_state field, must infer from messages)
    print("‚úÖ Prebuilt agent: State structure compatible")
```

**Step 3: Run evaluation tests**

Run: `cd agent-appoiments-v2 && source venv/bin/activate && pytest tests/test_prebuilt_evaluation.py -v -s`
Expected: Tests show performance comparison

**Step 4: DECISION POINT**

**IF prebuilt is within 20% of custom performance:**
- ‚úÖ Use prebuilt agent
- ‚úÖ Skip Tasks 3-6 (custom optimizations)
- ‚úÖ Jump to Task 7 (streaming)
- ‚úÖ Save 2 days of development

**IF prebuilt is >20% slower:**
- ‚ö†Ô∏è Continue with custom optimization
- ‚ö†Ô∏è Proceed to Task 3

**Step 5: Commit (regardless of decision)**

```bash
git add src/agent_prebuilt.py tests/test_prebuilt_evaluation.py
git commit -m "feat(optimization): evaluate prebuilt create_react_agent

- RECOMMENDED approach before custom optimization
- Battle-tested optimizations included
- Performance comparison test
- Decision criteria: within 20% = use prebuilt

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## üîß PHASE 2: ASYNC & STREAMING (IF CONTINUING CUSTOM)

### Task 3: Convert to Async Patterns (CRITICAL for 100+ users)

**Goal:** Enable concurrent request handling with async/await patterns

**Files:**
- Modify: `agent-appoiments-v2/src/agent.py:371-496` (agent_node ‚Üí async)
- Modify: `agent-appoiments-v2/src/tools.py` (all tools ‚Üí async)
- Modify: `agent-appoiments-v2/api_server.py` (add async endpoints)

**Step 1: Convert agent_node to async**

In `src/agent.py:371-496`, convert `agent_node` to async:

```python
async def agent_node(state: AppointmentState) -> dict[str, Any]:
    """
    Agent node - ASYNC version (v2.0).

    CRITICAL: Async enables concurrent request handling.
    Without async: 10 users = 30s queue time
    With async: 10 users = 3s concurrent execution
    """
    messages = state.get("messages", [])
    current = state.get("current_state", ConversationState.COLLECT_SERVICE)

    # Security check (sync operations OK for fast validation)
    if messages:
        last_msg = messages[-1]
        if hasattr(last_msg, 'content') and last_msg.content:
            text_content = extract_text_from_content(last_msg.content)
            scan = detector.scan(text_content)
            if not scan.is_safe:
                return {
                    "messages": [SystemMessage(
                        content="[SECURITY] Your message was flagged. Please rephrase."
                    )],
                }

    # Build prompt
    system_prompt = build_system_prompt(state)

    # Apply trim_messages
    if messages:
        trimmed_messages = trim_messages(
            messages,
            max_tokens=2000,
            strategy="last",
            token_counter=llm,
            allow_partial=False,
            start_on="human",
        )
    else:
        trimmed_messages = []

    # Add system message
    full_msgs = [SystemMessage(content=system_prompt)] + trimmed_messages

    # ‚úÖ ASYNC LLM CALL (critical change)
    response = await llm_with_tools.ainvoke(full_msgs)

    # Log token usage
    log_tokens(response, context="Agent Node (async)")

    # Confirmation number extraction (same logic)
    import re
    from langchain_core.messages import ToolMessage

    confirmation_number = None
    for msg in reversed(state.get("messages", [])[-10:]):
        if isinstance(msg, ToolMessage) or (hasattr(msg, 'type') and msg.type == 'tool'):
            if hasattr(msg, 'content') and isinstance(msg.content, str):
                if '[SUCCESS]' in msg.content and 'Confirmation:' in msg.content:
                    match = re.search(r'(APPT-\d+)', msg.content, re.IGNORECASE)
                    if match:
                        confirmation_number = match.group(1)
                        break

    if confirmation_number:
        response_text = response.content if hasattr(response, 'content') else str(response)
        if confirmation_number not in response_text:
            if hasattr(response, 'content'):
                response.content = f"{response.content}\n\n‚úÖ Confirmation Number: **{confirmation_number}**"

    # Initialize state if needed
    result = {"messages": [response]}
    if "current_state" not in state:
        result["current_state"] = ConversationState.COLLECT_SERVICE
        result["collected_data"] = {}
        result["available_slots"] = []
        result["retry_count"] = {}

    return result
```

**Step 2: Convert tools to async**

In `src/tools.py`, convert HTTP calls to async:

```python
"""Agent tools with async patterns (v2.0)."""
import re
import json
import httpx  # ‚Üê Replace requests with httpx for async
from datetime import datetime
from typing import Optional
from langchain_core.tools import tool
from src import config
from src.cache import validation_cache, availability_cache

# Create async HTTP client (reuse connection pool)
http_client = httpx.AsyncClient(timeout=5.0)


@tool
async def get_services_tool() -> str:
    """
    Get available services (ASYNC version).
    Returns [SERVICES] list with IDs and names.
    """
    try:
        response = await http_client.get(f"{config.MOCK_API_BASE_URL}/services")
        response.raise_for_status()
        data = response.json()

        if not data.get("success"):
            return "[ERROR] Failed to fetch services"

        services = data.get("services", [])
        if not services:
            return "[ERROR] No services available"

        # Format for LLM
        result = "[SERVICES] Available services:\n"
        for service in services:
            result += (
                f"- {service['name']} "
                f"(ID: {service['id']}, Duration: {service['duration_minutes']} min)\n"
            )

        return result

    except httpx.RequestError as e:
        return f"[ERROR] Could not connect to API: {str(e)}"
    except Exception as e:
        return f"[ERROR] Unexpected error: {str(e)}"


@tool
async def fetch_and_cache_availability_tool(service_id: str) -> str:
    """
    Fetch 30 days availability and cache (ASYNC version).
    Call IMMEDIATELY after user selects service.
    """
    try:
        # Get today's date
        from datetime import date
        today = date.today().isoformat()

        response = await http_client.get(
            f"{config.MOCK_API_BASE_URL}/availability",
            params={"service_id": service_id, "date_from": today}
        )
        response.raise_for_status()
        data = response.json()

        if not data.get("success"):
            return f"[ERROR] Failed to fetch availability: {data.get('message', 'Unknown error')}"

        slots = data.get("available_slots", [])

        # Cache results
        availability_cache.set(service_id, slots)

        return f"[SUCCESS] Cached {len(slots)} slots for next 30 days (service: {service_id})"

    except httpx.RequestError as e:
        return f"[ERROR] Could not connect to API: {str(e)}"
    except Exception as e:
        return f"[ERROR] Unexpected error: {str(e)}"


@tool
async def create_appointment_tool(
    service_id: str,
    date: str,
    start_time: str,
    client_name: str,
    client_email: str,
    client_phone: str
) -> str:
    """
    Create appointment (ASYNC version).
    Returns [SUCCESS] with confirmation number or [ERROR].
    """
    try:
        payload = {
            "service_id": service_id,
            "date": date,
            "start_time": start_time,
            "client_name": client_name,
            "client_email": client_email,
            "client_phone": client_phone
        }

        response = await http_client.post(
            f"{config.MOCK_API_BASE_URL}/appointments",
            json=payload
        )
        response.raise_for_status()
        data = response.json()

        if data.get("success"):
            confirmation = data.get("confirmation_number", "UNKNOWN")
            return (
                f"[SUCCESS] Appointment created!\n"
                f"Confirmation: {confirmation}\n"
                f"Service: {service_id}\n"
                f"Date: {date}\n"
                f"Time: {start_time}\n"
                f"Name: {client_name}"
            )
        else:
            return f"[ERROR] {data.get('message', 'Failed to create appointment')}"

    except httpx.RequestError as e:
        return f"[ERROR] Could not connect to API: {str(e)}"
    except Exception as e:
        return f"[ERROR] Unexpected error: {str(e)}"


# ‚úÖ Validation tools remain sync (fast, no I/O)
@tool
def validate_email_tool(email: str) -> str:
    """Validate email format (sync OK - no I/O)."""
    is_valid, message = validation_cache.validate_email(email)
    if is_valid:
        return f"[VALID] Email '{email}' is valid."
    else:
        return f"[INVALID] Email '{email}' is not valid. Please provide a valid email."


@tool
def validate_phone_tool(phone: str) -> str:
    """Validate phone number (sync OK - no I/O)."""
    is_valid, message = validation_cache.validate_phone(phone)
    if is_valid:
        return f"[VALID] Phone '{phone}' is valid."
    else:
        return f"[INVALID] Phone '{phone}' is not valid. Please provide at least 7 digits."


# Convert remaining tools (filter_and_show, cancel, reschedule, get_appointment) similarly
```

**Step 3: Update graph creation to support async**

In `src/agent.py`, modify `create_graph()`:

```python
def create_graph():
    """
    Create graph with async support (v2.0).

    CRITICAL: Graph nodes can be async functions.
    LangGraph will await them automatically.
    """
    from src.tools_with_retry import AsyncToolNodeWithRetry

    builder = StateGraph(AppointmentState)

    # Add async nodes
    builder.add_node("agent", agent_node)  # ‚Üê Now async
    builder.add_node("tools", AsyncToolNodeWithRetry(tools, max_retries=2))  # ‚Üê Async version

    # Edges (same as before)
    builder.add_edge(START, "agent")
    builder.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END,
        }
    )
    builder.add_edge("tools", "agent")

    # Compile
    checkpointer = MemorySaver()
    return builder.compile(checkpointer=checkpointer)
```

**Step 4: Create async tool node with retry**

Create `src/tools_with_retry.py` (async version):

```python
"""
Async tool node with retry and parallel execution (v2.0).
"""
import asyncio
import logging
from typing import Any, Dict, List, Set
from langchain_core.messages import ToolMessage
from langgraph.prebuilt import ToolNode as BaseToolNode

logger = logging.getLogger(__name__)


class AsyncToolNodeWithRetry(BaseToolNode):
    """
    Async tool node with inline retry and parallel execution.

    CRITICAL for 100+ concurrent users.
    """

    def __init__(self, tools: List, max_retries: int = 2):
        super().__init__(tools)
        self.max_retries = max_retries

    async def _execute_with_retry(self, tool_call: Dict[str, Any]) -> ToolMessage:
        """Execute single tool call with retry (async)."""
        tool_name = tool_call.get("name", "unknown")
        tool_call_id = tool_call.get("id", "unknown")

        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                # Execute tool (check if async)
                result = await self._run_tool_async(tool_call)

                # Check if result indicates error
                if isinstance(result, str) and '[ERROR]' in result:
                    is_user_error = (
                        'not found' in result.lower() or
                        ('invalid' in result.lower() and 'format' in result.lower())
                    )

                    is_system_error = (
                        'could not connect' in result.lower() or
                        'timeout' in result.lower()
                    )

                    if is_system_error:
                        logger.warning(f"System error in {tool_name}: {result}")
                        return ToolMessage(content=result, tool_call_id=tool_call_id)

                    if is_user_error and attempt < self.max_retries:
                        backoff = 0.5 * (2 ** attempt)
                        logger.info(f"Retrying {tool_name} after {backoff}s")
                        await asyncio.sleep(backoff)  # ‚Üê Async sleep
                        continue

                return ToolMessage(content=result, tool_call_id=tool_call_id)

            except Exception as e:
                last_error = str(e)
                if attempt < self.max_retries:
                    backoff = 0.5 * (2 ** attempt)
                    logger.warning(f"Exception in {tool_name}, retrying: {e}")
                    await asyncio.sleep(backoff)
                else:
                    logger.error(f"Max retries for {tool_name}: {e}")

        error_msg = f"[ERROR] Tool {tool_name} failed after {self.max_retries + 1} attempts: {last_error}"
        return ToolMessage(content=error_msg, tool_call_id=tool_call_id)

    async def _run_tool_async(self, tool_call: Dict[str, Any]) -> str:
        """Execute tool (async if possible, sync fallback)."""
        tool_name = tool_call["name"]
        tool_args = tool_call.get("args", {})

        for tool in self.tools:
            if tool.name == tool_name:
                # Check if tool is async
                if asyncio.iscoroutinefunction(tool.func):
                    return await tool.ainvoke(tool_args)
                else:
                    # Sync tool - run in executor to not block
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(None, tool.invoke, tool_args)

        return f"[ERROR] Tool {tool_name} not found"

    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute tool calls with async parallel execution.

        CRITICAL: Parallel execution for independent tools.
        """
        messages = state.get("messages", [])
        if not messages:
            return state

        last_message = messages[-1]
        if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
            return state

        tool_calls = last_message.tool_calls

        # Check if tools can run in parallel
        if len(tool_calls) > 1 and self._can_run_parallel(tool_calls):
            # PARALLEL EXECUTION (async)
            logger.info(f"‚ö° Executing {len(tool_calls)} tools in parallel (async)")

            # Execute all tools concurrently
            tasks = [self._execute_with_retry(tc) for tc in tool_calls]
            tool_messages = await asyncio.gather(*tasks, return_exceptions=False)
        else:
            # SEQUENTIAL EXECUTION
            logger.info(f"üîÑ Executing {len(tool_calls)} tools sequentially")
            tool_messages = []
            for tc in tool_calls:
                result = await self._execute_with_retry(tc)
                tool_messages.append(result)

        return {"messages": tool_messages}

    def _can_run_parallel(self, tool_calls: List[Dict]) -> bool:
        """Check if tools have dependencies."""
        tool_names = [tc["name"] for tc in tool_calls]

        dependencies = {
            "filter_and_show_availability_tool": {"fetch_and_cache_availability_tool"},
            "create_appointment_tool": {"validate_email_tool", "validate_phone_tool"},
        }

        for tool_call in tool_calls:
            deps = dependencies.get(tool_call["name"], set())
            if deps.intersection(tool_names):
                return False

        return True
```

**Step 5: Update API server with async endpoints**

In `api_server.py`, add async streaming endpoint:

```python
"""
FastAPI server with async streaming support (v2.0).
"""
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio

app = FastAPI()

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """
    Async chat endpoint with streaming support.

    CRITICAL for production: Non-blocking async execution.
    """
    graph = create_graph()

    # ‚úÖ Use recursion_limit in config (not manual iteration_count)
    config = {
        "configurable": {
            "thread_id": request.thread_id,
            "recursion_limit": 10  # ‚Üê Native LangGraph feature
        }
    }

    # Async invoke
    result = await graph.ainvoke(
        {"messages": request.messages},
        config=config
    )

    return {"messages": result["messages"]}


@app.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """
    Streaming endpoint with stream_mode="updates" (v2.0).

    CRITICAL: Use "updates" mode for efficiency (not "values").
    """
    graph = create_graph()

    config = {
        "configurable": {
            "thread_id": request.thread_id,
            "recursion_limit": 10
        }
    }

    async def event_generator():
        """Generate SSE events from graph stream."""
        async for chunk in graph.astream(
            {"messages": request.messages},
            config=config,
            stream_mode="updates"  # ‚Üê CRITICAL: Only send changes, not full state
        ):
            # Chunk format: {"node_name": {"messages": [...]}}
            for node_name, node_output in chunk.items():
                if "messages" in node_output:
                    for msg in node_output["messages"]:
                        yield f"data: {json.dumps({'node': node_name, 'message': msg.content})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )
```

**Step 6: Create async tests**

Create `tests/test_async_patterns.py`:

```python
"""Test async patterns."""
import pytest
import asyncio
import time
from langchain_core.messages import HumanMessage
from src.agent import create_graph

@pytest.mark.asyncio
async def test_async_invoke():
    """Test async invocation works."""
    graph = create_graph()
    config = {"configurable": {"thread_id": "test-async", "recursion_limit": 10}}

    result = await graph.ainvoke(
        {"messages": [HumanMessage(content="Hello")]},
        config=config
    )

    assert "messages" in result
    assert len(result["messages"]) > 0


@pytest.mark.asyncio
async def test_concurrent_requests():
    """
    Test that async enables true concurrency.

    CRITICAL: 10 concurrent requests should take ~3s (not 30s).
    """
    graph = create_graph()

    async def single_request(user_id: int):
        """Single user request."""
        config = {"configurable": {"thread_id": f"user-{user_id}", "recursion_limit": 10}}
        result = await graph.ainvoke(
            {"messages": [HumanMessage(content="Book appointment")]},
            config=config
        )
        return result

    # 10 concurrent users
    start = time.time()
    tasks = [single_request(i) for i in range(10)]
    results = await asyncio.gather(*tasks)
    elapsed = time.time() - start

    print(f"\n‚ö° 10 concurrent requests: {elapsed:.2f}s")

    # Should be closer to 3s (single request) than 30s (sequential)
    assert elapsed < 10, f"Concurrent execution took {elapsed:.2f}s (expected <10s)"
    assert len(results) == 10


@pytest.mark.asyncio
async def test_streaming_updates_mode():
    """Test streaming with stream_mode='updates'."""
    graph = create_graph()
    config = {"configurable": {"thread_id": "test-stream", "recursion_limit": 10}}

    chunks_received = 0
    async for chunk in graph.astream(
        {"messages": [HumanMessage(content="Hello")]},
        config=config,
        stream_mode="updates"  # ‚Üê Should only send changes
    ):
        chunks_received += 1
        # Each chunk should be {node_name: {partial_state}}
        assert isinstance(chunk, dict)
        print(f"Chunk {chunks_received}: {list(chunk.keys())}")

    assert chunks_received > 0, "No chunks received from stream"
    print(f"‚úÖ Received {chunks_received} update chunks")
```

**Step 7: Run async tests**

Run: `pytest tests/test_async_patterns.py -v -s`
Expected: All tests pass, concurrent test shows ~3s (not 30s)

**Step 8: Commit**

```bash
git add src/agent.py src/tools.py src/tools_with_retry.py api_server.py tests/test_async_patterns.py
git commit -m "feat(optimization): convert to async patterns for concurrency

- async/await throughout (agent_node, tools, graph)
- httpx for async HTTP requests
- AsyncToolNodeWithRetry with asyncio.gather
- Enables 10 concurrent users in ~3s (not 30s sequential)
- CRITICAL for 100+ user scalability

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com)"
```

---

### Task 4: Implement Proper Streaming with stream_mode="updates"

**Goal:** Add production-ready streaming for better UX and efficiency

**Files:**
- Modify: `agent-appoiments-v2/api_server.py` (enhance streaming)
- Create: `agent-appoiments-v2/src/streaming.py` (streaming utilities)
- Create: `tests/test_streaming.py`

**Step 1: Create streaming utilities**

Create `src/streaming.py`:

```python
"""
Streaming utilities for LangGraph (v2.0).

Best practices:
- Use stream_mode="updates" for efficiency (not "values")
- Handle partial messages gracefully
- Provide progress indicators
"""
import json
import asyncio
from typing import AsyncGenerator, Dict, Any
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage


async def stream_graph_updates(
    graph,
    initial_state: Dict[str, Any],
    config: Dict[str, Any]
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Stream graph execution with proper formatting.

    Args:
        graph: Compiled LangGraph
        initial_state: Initial state with messages
        config: Config with thread_id and recursion_limit

    Yields:
        Dict with event type and data
    """
    async for chunk in graph.astream(
        initial_state,
        config=config,
        stream_mode="updates"  # ‚Üê CRITICAL: Only changes, not full state
    ):
        # Chunk format: {node_name: {partial_state}}
        for node_name, node_output in chunk.items():
            # Yield progress event
            yield {
                "type": "node_start",
                "node": node_name,
                "timestamp": asyncio.get_event_loop().time()
            }

            # Handle messages
            if "messages" in node_output:
                for msg in node_output["messages"]:
                    if isinstance(msg, AIMessage):
                        # Stream AI response
                        yield {
                            "type": "message",
                            "role": "assistant",
                            "content": msg.content,
                            "tool_calls": msg.tool_calls if hasattr(msg, 'tool_calls') and msg.tool_calls else None
                        }
                    elif isinstance(msg, ToolMessage):
                        # Stream tool result
                        yield {
                            "type": "tool_result",
                            "tool_call_id": msg.tool_call_id if hasattr(msg, 'tool_call_id') else None,
                            "content": msg.content
                        }

            # Yield progress completion
            yield {
                "type": "node_complete",
                "node": node_name,
                "timestamp": asyncio.get_event_loop().time()
            }


def format_sse_event(data: Dict[str, Any]) -> str:
    """
    Format dict as Server-Sent Event.

    Args:
        data: Event data

    Returns:
        SSE-formatted string
    """
    return f"data: {json.dumps(data)}\n\n"


async def stream_with_progress(
    graph,
    initial_state: Dict[str, Any],
    config: Dict[str, Any]
) -> AsyncGenerator[str, None]:
    """
    Stream with progress indicators (SSE format).

    Yields:
        SSE-formatted events
    """
    async for event in stream_graph_updates(graph, initial_state, config):
        yield format_sse_event(event)
```

**Step 2: Enhance API server with proper streaming**

In `api_server.py`, update streaming endpoint:

```python
from src.streaming import stream_with_progress

@app.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """
    Production streaming endpoint with proper formatting.

    Uses stream_mode="updates" for efficiency.
    Returns SSE (Server-Sent Events) format.
    """
    graph = create_graph()

    config = {
        "configurable": {
            "thread_id": request.thread_id,
            "recursion_limit": 10  # ‚Üê Native LangGraph limit
        }
    }

    return StreamingResponse(
        stream_with_progress(
            graph,
            {"messages": request.messages},
            config
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )
```

**Step 3: Create streaming tests**

Create `tests/test_streaming.py`:

```python
"""Test streaming functionality."""
import pytest
import asyncio
from langchain_core.messages import HumanMessage
from src.agent import create_graph
from src.streaming import stream_graph_updates

@pytest.mark.asyncio
async def test_stream_mode_updates():
    """Test that stream_mode='updates' only sends changes."""
    graph = create_graph()
    config = {"configurable": {"thread_id": "test-stream-updates", "recursion_limit": 10}}

    initial_state = {"messages": [HumanMessage(content="Hello")]}

    events = []
    async for event in stream_graph_updates(graph, initial_state, config):
        events.append(event)

    # Should receive multiple events (node_start, message, node_complete, etc.)
    assert len(events) > 0

    # Check event types
    event_types = {e["type"] for e in events}
    assert "message" in event_types or "tool_result" in event_types

    print(f"‚úÖ Received {len(events)} streaming events")


@pytest.mark.asyncio
async def test_streaming_shows_progress():
    """Test that streaming provides progress indicators."""
    graph = create_graph()
    config = {"configurable": {"thread_id": "test-progress", "recursion_limit": 10}}

    initial_state = {"messages": [HumanMessage(content="Book appointment")]}

    node_starts = []
    node_completes = []

    async for event in stream_graph_updates(graph, initial_state, config):
        if event["type"] == "node_start":
            node_starts.append(event["node"])
        elif event["type"] == "node_complete":
            node_completes.append(event["node"])

    # Should show progress through nodes
    assert len(node_starts) > 0
    assert len(node_completes) > 0

    print(f"‚úÖ Nodes executed: {node_starts}")


@pytest.mark.asyncio
async def test_streaming_faster_than_invoke():
    """
    Test that streaming APPEARS faster (first token).

    Note: Total time is same, but user sees first response sooner.
    """
    import time
    graph = create_graph()
    config = {"configurable": {"thread_id": "test-ttft", "recursion_limit": 10}}

    initial_state = {"messages": [HumanMessage(content="Hello")]}

    # Measure time to first token (streaming)
    start_stream = time.time()
    first_token_time = None
    async for event in stream_graph_updates(graph, initial_state, config):
        if event["type"] == "message" and first_token_time is None:
            first_token_time = time.time() - start_stream
            break

    # Measure full invoke time
    start_invoke = time.time()
    result = await graph.ainvoke(initial_state, config)
    invoke_time = time.time() - start_invoke

    print(f"\n‚ö° Streaming first token: {first_token_time:.2f}s")
    print(f"üì¶ Invoke full response: {invoke_time:.2f}s")

    # First token should be significantly faster
    assert first_token_time < invoke_time * 0.5, \
        f"First token ({first_token_time:.2f}s) not faster than invoke ({invoke_time:.2f}s)"
```

**Step 4: Run streaming tests**

Run: `pytest tests/test_streaming.py -v -s`
Expected: All tests pass

**Step 5: Commit**

```bash
git add src/streaming.py api_server.py tests/test_streaming.py
git commit -m "feat(optimization): implement production streaming

- stream_mode='updates' for efficiency (not 'values')
- SSE format with progress indicators
- Time-to-first-token optimization
- Better UX: users see responses immediately

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com)"
```

---

## üéØ PHASE 3: NATIVE LANGGRAPH FEATURES

### Task 5: Use recursion_limit (Native Feature, Not Manual)

**Goal:** Replace manual iteration_count with LangGraph's native recursion_limit

**Files:**
- Modify: `agent-appoiments-v2/src/state.py` (REMOVE iteration_count)
- Modify: `agent-appoiments-v2/src/agent.py` (REMOVE manual checking)
- Modify: `agent-appoiments-v2/api_server.py` (ADD recursion_limit to config)
- Modify: All test files (ADD recursion_limit to config)

**Step 1: Remove manual iteration_count from state**

In `src/state.py`, remove the field:

```python
from typing import Annotated, TypedDict
from langgraph.graph import add_messages
from enum import Enum

class ConversationState(Enum):
    """Conversation states."""
    # ... (existing states)

class AppointmentState(TypedDict):
    """
    State for appointment booking (v2.0 - CLEANED).

    REMOVED: iteration_count (use recursion_limit in config instead)
    """
    messages: Annotated[list, add_messages]
    current_state: ConversationState
    collected_data: dict
    available_slots: list
    retry_count: dict
    # ‚ùå REMOVED: iteration_count: int  (use recursion_limit instead)
```

**Step 2: Remove manual iteration checking from agent_node**

In `src/agent.py:371-496`, remove all iteration_count logic:

```python
async def agent_node(state: AppointmentState) -> dict[str, Any]:
    """
    Agent node - v2.0 CLEANED.

    REMOVED: Manual iteration_count checking.
    LangGraph handles this via recursion_limit in config.

    If recursion_limit is exceeded, LangGraph raises GraphRecursionError.
    """
    messages = state.get("messages", [])
    current = state.get("current_state", ConversationState.COLLECT_SERVICE)

    # ‚ùå REMOVED: Manual iteration checking
    # iteration_count = state.get("iteration_count", 0)
    # if iteration_count >= MAX_ITERATIONS:
    #     return fallback_message

    # Security check
    if messages:
        last_msg = messages[-1]
        if hasattr(last_msg, 'content') and last_msg.content:
            text_content = extract_text_from_content(last_msg.content)
            scan = detector.scan(text_content)
            if not scan.is_safe:
                return {
                    "messages": [SystemMessage(
                        content="[SECURITY] Your message was flagged."
                    )],
                }

    # Build prompt
    system_prompt = build_system_prompt(state)

    # Trim messages
    if messages:
        trimmed_messages = trim_messages(
            messages,
            max_tokens=2000,
            strategy="last",
            token_counter=llm,
            allow_partial=False,
            start_on="human",
        )
    else:
        trimmed_messages = []

    full_msgs = [SystemMessage(content=system_prompt)] + trimmed_messages

    # Async LLM call
    response = await llm_with_tools.ainvoke(full_msgs)

    # Log token usage
    log_tokens(response, context="Agent Node (async)")

    # Confirmation number extraction (same as before)
    # ... (confirmation logic)

    # Return result WITHOUT iteration_count
    result = {"messages": [response]}

    # Initialize state if needed
    if "current_state" not in state:
        result["current_state"] = ConversationState.COLLECT_SERVICE
        result["collected_data"] = {}
        result["available_slots"] = []
        result["retry_count"] = {}
    # ‚ùå REMOVED: result["iteration_count"] = iteration_count + 1

    return result
```

**Step 3: Add recursion_limit to all API endpoints**

In `api_server.py`, ensure all endpoints use recursion_limit:

```python
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """Chat endpoint with proper recursion_limit."""
    graph = create_graph()

    # ‚úÖ CORRECT: recursion_limit in config
    config = {
        "configurable": {
            "thread_id": request.thread_id,
            "recursion_limit": 10  # ‚Üê Native LangGraph feature
        }
    }

    try:
        result = await graph.ainvoke(
            {"messages": request.messages},
            config=config
        )
        return {"messages": result["messages"]}

    except GraphRecursionError as e:
        # ‚úÖ Handle recursion limit gracefully
        return {
            "error": "Maximum conversation iterations reached",
            "message": "Please provide more complete information or start a new booking.",
            "type": "recursion_limit_exceeded"
        }


@app.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """Streaming endpoint with recursion_limit."""
    graph = create_graph()

    config = {
        "configurable": {
            "thread_id": request.thread_id,
            "recursion_limit": 10  # ‚Üê Same limit
        }
    }

    return StreamingResponse(
        stream_with_progress(graph, {"messages": request.messages}, config),
        media_type="text/event-stream"
    )
```

**Step 4: Update all test files to use recursion_limit**

Search and replace in all test files:

```bash
# Find all test files
grep -r "\"thread_id\"" tests/ --include="*.py"

# Replace pattern:
# OLD: {"configurable": {"thread_id": "test-123"}}
# NEW: {"configurable": {"thread_id": "test-123", "recursion_limit": 10}}
```

Example in `tests/test_async_patterns.py`:

```python
@pytest.mark.asyncio
async def test_async_invoke():
    """Test with proper recursion_limit."""
    graph = create_graph()

    # ‚úÖ CORRECT: recursion_limit in config
    config = {
        "configurable": {
            "thread_id": "test-async",
            "recursion_limit": 10  # ‚Üê Add to ALL tests
        }
    }

    result = await graph.ainvoke(
        {"messages": [HumanMessage(content="Hello")]},
        config=config
    )

    assert "messages" in result
```

**Step 5: Create test for recursion_limit behavior**

Create `tests/test_recursion_limit.py`:

```python
"""Test native recursion_limit feature."""
import pytest
from langgraph.errors import GraphRecursionError
from langchain_core.messages import HumanMessage
from src.agent import create_graph

@pytest.mark.asyncio
async def test_recursion_limit_enforced():
    """
    Test that LangGraph enforces recursion_limit natively.

    This replaces manual iteration_count checking.
    """
    graph = create_graph()

    # Set LOW recursion limit to trigger error
    config = {
        "configurable": {
            "thread_id": "test-recursion",
            "recursion_limit": 2  # ‚Üê Very low to test enforcement
        }
    }

    # This should hit recursion limit
    with pytest.raises(GraphRecursionError):
        result = await graph.ainvoke(
            {"messages": [HumanMessage(content="gibberish input that causes many iterations")]},
            config=config
        )

    print("‚úÖ GraphRecursionError raised as expected")


@pytest.mark.asyncio
async def test_normal_flow_within_limit():
    """Test that normal flow completes within limit."""
    graph = create_graph()

    config = {
        "configurable": {
            "thread_id": "test-normal",
            "recursion_limit": 10  # ‚Üê Reasonable limit
        }
    }

    # Should complete without hitting limit
    result = await graph.ainvoke(
        {"messages": [HumanMessage(content="Hello")]},
        config=config
    )

    assert "messages" in result
    print("‚úÖ Completed within recursion_limit")


@pytest.mark.asyncio
async def test_recursion_limit_per_request():
    """Test that recursion_limit is per-request, not global."""
    graph = create_graph()

    # Request 1: Low limit (will fail)
    config1 = {"configurable": {"thread_id": "user-1", "recursion_limit": 2}}

    try:
        await graph.ainvoke(
            {"messages": [HumanMessage(content="complex request")]},
            config1
        )
    except GraphRecursionError:
        pass  # Expected

    # Request 2: Normal limit (should succeed)
    config2 = {"configurable": {"thread_id": "user-2", "recursion_limit": 10}}

    result = await graph.ainvoke(
        {"messages": [HumanMessage(content="simple request")]},
        config2
    )

    assert "messages" in result
    print("‚úÖ recursion_limit is per-request (not global)")
```

**Step 6: Run recursion_limit tests**

Run: `pytest tests/test_recursion_limit.py -v -s`
Expected: All tests pass, GraphRecursionError raised when appropriate

**Step 7: Run full test suite with new config**

Run: `timeout 180 pytest tests/ -v --tb=short`
Expected: All tests pass with recursion_limit in config

**Step 8: Commit**

```bash
git add src/state.py src/agent.py api_server.py tests/
git commit -m "refactor(optimization): use native recursion_limit (not manual)

- Removed manual iteration_count from state
- Removed manual checking in agent_node
- Use recursion_limit in config (native LangGraph feature)
- GraphRecursionError handled gracefully
- Cleaner code, less overhead

CRITICAL FIX: Follows LangGraph best practices

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com)"
```

---

## üìà PHASE 4: VALIDATION & DOCUMENTATION

### Task 6: Final Performance Validation

**Goal:** Measure and document final improvements

**Files:**
- Create: `docs/OPTIMIZATION_RESULTS_v2.md`
- Update: `scripts/measure_baseline.py` (final measurement)

**Step 1: Run comprehensive final measurement**

Run: `python scripts/measure_baseline.py` (after 100+ conversations)

**Step 2: Create results document**

Create `docs/OPTIMIZATION_RESULTS_v2.md`:

```markdown
# Optimization Results Report (v2.0 - CORRECTED)

**Date:** [INSERT DATE]
**Critical Fixes Applied:**
- ‚úÖ Native recursion_limit (not manual iteration_count)
- ‚úÖ Async/await patterns throughout
- ‚úÖ Streaming with stream_mode="updates"
- ‚úÖ create_react_agent evaluated FIRST
- ‚úÖ Production-ready for 100+ concurrent users

---

## Executive Summary

**Goal:** Reduce latency from 3.9s/7.2 iterations to 1.5-2s/2-3 iterations

**Results:**
- Average latency: [INSERT]ms (target: <2000ms)
- Average iterations: [INSERT] (target: 2-3)
- Max iterations: 10 (enforced via recursion_limit)
- P95 latency: [INSERT]ms (target: <3000ms)
- Concurrent capacity: 100+ users (async enabled)

**Status:** [ACHIEVED / IN PROGRESS]

---

## Critical Fixes vs v1.0

| Issue | v1.0 (INCORRECT) | v2.0 (CORRECTED) |
|-------|------------------|------------------|
| **Iteration Limit** | Manual iteration_count in state | recursion_limit in config |
| **Concurrency** | Sync invoke() | Async ainvoke() |
| **Streaming** | Vague implementation | stream_mode="updates" |
| **Architecture** | Custom optional | Prebuilt evaluated first |
| **Scalability** | 1 user at a time | 100+ concurrent |

---

## Baseline vs Optimized

| Metric | Baseline (v1.x) | Optimized (v2.0) | Improvement |
|--------|-----------------|------------------|-------------|
| Avg Latency | 3,860ms | [INSERT]ms | [INSERT]% |
| Median Latency | 2,686ms | [INSERT]ms | [INSERT]% |
| Avg Iterations | 7.2 | [INSERT] | [INSERT]% |
| Max Iterations | 15 | 10 (enforced) | 33% |
| Concurrent Users | 1 (blocking) | 100+ (async) | ‚àû% |
| Time-to-First-Token | N/A (no streaming) | [INSERT]ms | NEW |

---

## Architecture Decisions

### Decision 1: Prebuilt vs Custom Agent

**Evaluation Result:**
- [IF PREBUILT USED] ‚úÖ create_react_agent selected - performance competitive, less code
- [IF CUSTOM USED] ‚ö†Ô∏è Custom agent required - prebuilt was [X]% slower

### Decision 2: Async Patterns

**Result:** ‚úÖ Async enabled
- Concurrent capacity: 1 ‚Üí 100+ users
- 10 concurrent requests: ~3s (not 30s sequential)

### Decision 3: Streaming Mode

**Result:** ‚úÖ stream_mode="updates" implemented
- Only sends changes (not full state)
- Time-to-first-token: [INSERT]ms
- Better UX: users see responses immediately

---

## Production Readiness

### ‚úÖ Achieved
- [x] Async/await for concurrency
- [x] Native recursion_limit (not manual)
- [x] Streaming with proper mode
- [x] Performance regression tests
- [x] Error handling (GraphRecursionError)

### üìã Remaining
- [ ] Load testing with 100+ concurrent users
- [ ] OpenAI rate limit monitoring
- [ ] Redis for distributed checkpointing
- [ ] Horizontal scaling (multiple pods)

---

## Cost Analysis

**Baseline:**
- 7.2 calls/conversation √ó ~190 tokens/call = 1,368 tokens
- Cost: ~$0.000657/conversation

**Optimized:**
- [INSERT] calls/conversation √ó ~190 tokens/call = [INSERT] tokens
- Cost: ~$[INSERT]/conversation

**Savings:** [INSERT]% reduction

---

## Next Steps

1. **Staging Deployment:** Deploy to staging with 10% traffic
2. **Load Testing:** Simulate 100 concurrent users
3. **Monitoring:** Set up latency/error dashboards
4. **Gradual Rollout:** 10% ‚Üí 50% ‚Üí 100%

---

**Report Generated:** [DATE]
```

**Step 3: Commit final documentation**

```bash
git add docs/OPTIMIZATION_RESULTS_v2.md
git commit -m "docs(optimization): final results report (v2.0 corrected)

- Critical fixes applied (recursion_limit, async, streaming)
- Production-ready for 100+ concurrent users
- Prebuilt vs custom decision documented

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com)"
```

---

## üéØ SUMMARY OF CRITICAL FIXES

### ‚úÖ Fixed #1: recursion_limit (Native, Not Manual)
**Before (v1.0 - WRONG):**
```python
class AppointmentState(TypedDict):
    iteration_count: int  # ‚ùå Manual tracking

def agent_node(state):
    if state["iteration_count"] >= 3:
        return fallback
```

**After (v2.0 - CORRECT):**
```python
# ‚úÖ Use native LangGraph feature
config = {"configurable": {"recursion_limit": 10}}
result = await graph.ainvoke(state, config=config)
```

### ‚úÖ Fixed #2: Async/Await (Enables Concurrency)
**Before (v1.0 - WRONG):**
```python
def agent_node(state):
    response = llm.invoke(messages)  # ‚ùå Blocking
```

**After (v2.0 - CORRECT):**
```python
async def agent_node(state):
    response = await llm.ainvoke(messages)  # ‚úÖ Non-blocking
```

### ‚úÖ Fixed #3: Streaming Mode (Efficient)
**Before (v1.0 - VAGUE):**
```python
# ‚ùå No specific mode
for chunk in graph.stream(...):
    yield chunk
```

**After (v2.0 - CORRECT):**
```python
# ‚úÖ Use "updates" mode
async for chunk in graph.astream(..., stream_mode="updates"):
    yield chunk
```

### ‚úÖ Fixed #4: Prebuilt Agent (Evaluate First)
**Before (v1.0 - OPTIONAL):**
- Task 7: "This is OPTIONAL, only if custom fails"

**After (v2.0 - RECOMMENDED):**
- Task 2: "Evaluate prebuilt FIRST before custom optimization"

### ‚úÖ Fixed #5: Concurrent Scaling
**Before (v1.0 - BLOCKING):**
- 10 users = 30s queue time (sequential)

**After (v2.0 - ASYNC):**
- 10 users = 3s concurrent execution

---

## üìö REFERENCES

- **LangGraph Recursion Limit:** https://langchain-ai.github.io/langgraph/concepts/low_level/#recursion-limit
- **Streaming Modes:** https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming
- **create_react_agent:** https://langchain-ai.github.io/langgraph/reference/prebuilt/#create_react_agent
- **Async Patterns:** https://langchain-ai.github.io/langgraph/how-tos/async/

---

**END OF CORRECTED PLAN (v2.0)**

*Estimated Time: 2-3 days*
*Expected Improvement: 50-60% latency reduction + 100x concurrency*
